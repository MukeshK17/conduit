# Conduit Configuration
# See docs/configuration.md for detailed documentation

routing:
  # Default optimization strategy
  # Options: balanced, quality, cost, speed
  default_optimization: balanced

  # Reward weight presets for routing decisions
  # Each preset balances quality, cost, and latency differently
  # Weights must sum to 1.0
  presets:
    balanced:
      quality: 0.7    # Prioritize quality
      cost: 0.2       # Moderate cost concern
      latency: 0.1    # Low latency concern

    quality:
      quality: 0.8    # Maximize quality
      cost: 0.1       # Minimal cost concern
      latency: 0.1    # Minimal latency concern

    cost:
      quality: 0.4    # Acceptable quality
      cost: 0.5       # Minimize cost
      latency: 0.1    # Low latency concern

    speed:
      quality: 0.4    # Acceptable quality
      cost: 0.1       # Low cost concern
      latency: 0.5    # Minimize latency

# Industry-wide priors for cold start optimization
#
# Data Sources (see docs/PRIORS.md for full methodology):
# - Artificial Analysis: https://artificialanalysis.ai/leaderboards/providers
# - Vellum LLM Leaderboard: https://www.vellum.ai/llm-leaderboard
#
# Format: model_id: [alpha, beta] where Beta(alpha, beta) represents prior belief
# Quality estimate = alpha / (alpha + beta)
# Prior strength = alpha + beta (10000 = high confidence)
#
# To update priors from latest benchmarks:
#   python scripts/sync_priors.py --config conduit.yaml
#
priors:
  # Code Context: Code generation, debugging, technical queries
  # Primary benchmark: SWE-Bench (Vellum), LiveCodeBench (Artificial Analysis)
  code:
    gpt-4o: [8000, 2000]              # 80% - Strong coding, based on LiveCodeBench
    gpt-4o-mini: [7200, 2800]         # 72% - Good for simple code tasks
    claude-3-5-sonnet-20241022: [8200, 1800]  # 82% - SWE-Bench 82% (Vellum)
    claude-3-opus-20240229: [8090, 1910]      # 80.9% - SWE-Bench 80.9% (Vellum)

  # Creative Context: Creative writing, storytelling, content generation
  # Primary benchmark: Qualitative assessments, community consensus
  # Note: No standardized creative benchmark; estimates based on model capabilities
  creative:
    gpt-4o: [7800, 2200]              # 78% - Good creative output
    gpt-4o-mini: [6500, 3500]         # 65% - Acceptable for drafts
    claude-3-5-sonnet-20241022: [8800, 1200]  # 88% - Known for nuanced writing
    claude-3-opus-20240229: [9200, 800]       # 92% - Best creative quality

  # Analysis Context: Analytical reasoning, comparison, evaluation
  # Primary benchmark: GPQA Diamond (Vellum), Intelligence Index (AA)
  analysis:
    gpt-4o: [7500, 2500]              # 75% - GPQA ~75% (Vellum)
    gpt-4o-mini: [6000, 4000]         # 60% - Limited reasoning depth
    claude-3-5-sonnet-20241022: [7800, 2200]  # 78% - Strong reasoning
    claude-3-opus-20240229: [8700, 1300]      # 87% - GPQA 87% (Vellum)

  # Simple QA Context: Factual questions, straightforward queries
  # Primary benchmark: MMLU Pro (Artificial Analysis)
  simple_qa:
    gpt-4o: [8200, 1800]              # 82% - MMLU Pro ~82% (AA)
    gpt-4o-mini: [6800, 3200]         # 68% - MMLU Pro ~68% (AA)
    claude-3-5-sonnet-20241022: [8750, 1250]  # 87.5% - MMLU Pro 87.5% (AA)
    claude-3-opus-20240229: [8950, 1050]      # 89.5% - MMLU Pro 89.5% (AA)

  # General Context: Default for unclassified queries
  # Weighted average across benchmarks (see docs/PRIORS.md for weights)
  general:
    gpt-4o: [7900, 2100]              # 79% - Balanced performance
    gpt-4o-mini: [6600, 3400]         # 66% - Cost-effective baseline
    claude-3-5-sonnet-20241022: [8300, 1700]  # 83% - Strong all-around
    claude-3-opus-20240229: [8700, 1300]      # 87% - Premium quality
